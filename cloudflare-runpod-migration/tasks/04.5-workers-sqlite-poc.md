# Task 4.5: Workers SQLite Proof-of-Concept

## Overview
Validate the in-memory SQLite strategy using sql.js (WebAssembly) before building the full Workers API. This proves the core data approach works and establishes real performance thresholds.

## Owner
**Claude** - Code generation and benchmarking

## Prerequisites
- Task 01 complete (R2 bucket exists)
- Node.js installed locally

## Why This Matters
The entire Workers architecture depends on loading SQLite from R2 into memory via sql.js. If this is slow or has edge cases, we need to know **before** building RunPod integration or Workers APIs on top of it.

---

## Threshold Analysis

### The Question
At what database size should we migrate to Durable Objects?

### Factors

**1. R2 Latency (network transfer)**
| DB Size | Download Time | Upload Time |
|---------|---------------|-------------|
| 100KB | ~30-50ms | ~30-50ms |
| 500KB | ~50-100ms | ~50-100ms |
| 1MB | ~100-200ms | ~100-200ms |
| 2MB | ~150-300ms | ~150-300ms |
| 5MB | ~300-500ms | ~300-500ms |

**2. sql.js Parse Time (CPU)**
| DB Size | Parse Time |
|---------|------------|
| 100KB | ~10-20ms |
| 500KB | ~30-50ms |
| 1MB | ~50-100ms |
| 2MB | ~100-200ms |
| 5MB | ~200-400ms |

**3. Total Request Overhead**
| DB Size | Download + Parse | Upload + Serialize |
|---------|------------------|-------------------|
| 100KB | ~50-70ms | ~40-60ms |
| 500KB | ~80-150ms | ~60-120ms |
| 1MB | ~150-300ms | ~120-250ms |
| 2MB | ~250-500ms | ~200-400ms |
| 5MB | ~500-900ms | ~400-700ms |

**4. Sync Frequency**
- **Downloads**: Only when R2 version > local version (rare for single-user)
- **Uploads**: Once per request with writes (every save operation)

Most requests won't re-download (version caching). Uploads are the main cost.

**5. User Experience Targets**
- Snappy: <100ms overhead
- Acceptable: <300ms overhead
- Noticeable lag: 300-500ms overhead
- Frustrating: >500ms overhead

### Better Approach: Measure Actual Delay

Instead of using file size as a proxy, **measure the actual sync time** and warn when it's too slow. This accounts for:
- Variable network conditions
- R2 performance fluctuations
- Different hardware/CPU speeds
- Real user experience

### Delay-Based Thresholds

| Sync Time | Status | Action |
|-----------|--------|--------|
| <100ms | Optimal | No action |
| 100-300ms | Acceptable | No action |
| 300-500ms | Warning | Log warning, consider archiving |
| >500ms | Critical | Show user warning, recommend archiving |

### Recommendation

```python
# Delay-based thresholds (measure actual sync time)
SYNC_TIME_OPTIMAL_MS = 100       # Feels instant
SYNC_TIME_ACCEPTABLE_MS = 300    # Still fine
SYNC_TIME_WARNING_MS = 500       # Log warning
SYNC_TIME_CRITICAL_MS = 1000     # Show user warning

# Track recent sync times for trend detection
SYNC_TIME_WINDOW = 10            # Number of recent syncs to track
SYNC_TIME_WARNING_RATIO = 0.3    # Warn if 30% of recent syncs are slow
```

**Key insight**: File size doesn't matter - delay does. A 2MB database on a fast connection might sync in 200ms (fine), while a 500KB database on a slow connection might take 600ms (problem). Measure what the user actually experiences.

---

## Implementation Steps

### 1. Create Minimal Workers Project

```bash
mkdir workers-sqlite-poc
cd workers-sqlite-poc
npm init -y
npm install sql.js wrangler
```

### 2. Create wrangler.toml

```toml
name = "sqlite-poc"
main = "src/index.ts"
compatibility_date = "2024-01-01"

[[r2_buckets]]
binding = "USER_DATA"
bucket_name = "reel-ballers-users"
```

### 3. Implement POC (src/index.ts)

```typescript
import initSqlJs, { Database } from 'sql.js';

interface Env {
  USER_DATA: R2Bucket;
}

// Delay-based thresholds (what matters is user-perceived delay)
const SYNC_TIME_OPTIMAL_MS = 100;
const SYNC_TIME_ACCEPTABLE_MS = 300;
const SYNC_TIME_WARNING_MS = 500;
const SYNC_TIME_CRITICAL_MS = 1000;

// Cache the SQL.js initialization
let SQL: Awaited<ReturnType<typeof initSqlJs>> | null = null;

async function getSqlJs() {
  if (!SQL) {
    SQL = await initSqlJs({
      // Use CDN for WASM file in Workers
      locateFile: (file: string) => `https://sql.js.org/dist/${file}`
    });
  }
  return SQL;
}

interface LoadResult {
  db: Database;
  timing: {
    total_ms: number;
    wasm_init_ms: number;
    download_ms: number;
    parse_ms: number;
  };
  size_bytes: number;
}

async function loadDatabase(bucket: R2Bucket, key: string): Promise<LoadResult> {
  const start = performance.now();

  const wasmStart = performance.now();
  const SQL = await getSqlJs();
  const wasm_init_ms = performance.now() - wasmStart;

  const downloadStart = performance.now();
  const object = await bucket.get(key);
  const download_ms = performance.now() - downloadStart;

  let db: Database;
  let parse_ms = 0;
  let size_bytes = 0;

  if (object) {
    const parseStart = performance.now();
    const buffer = await object.arrayBuffer();
    size_bytes = buffer.byteLength;
    db = new SQL.Database(new Uint8Array(buffer));
    parse_ms = performance.now() - parseStart;
  } else {
    db = new SQL.Database();
  }

  return {
    db,
    timing: {
      total_ms: performance.now() - start,
      wasm_init_ms,
      download_ms,
      parse_ms,
    },
    size_bytes,
  };
}

interface SaveResult {
  timing: {
    total_ms: number;
    serialize_ms: number;
    upload_ms: number;
  };
  size_bytes: number;
}

async function saveDatabase(bucket: R2Bucket, key: string, db: Database): Promise<SaveResult> {
  const start = performance.now();

  const serializeStart = performance.now();
  const data = db.export();
  const serialize_ms = performance.now() - serializeStart;

  const uploadStart = performance.now();
  await bucket.put(key, data.buffer, {
    customMetadata: {
      'db-version': Date.now().toString()
    }
  });
  const upload_ms = performance.now() - uploadStart;

  return {
    timing: {
      total_ms: performance.now() - start,
      serialize_ms,
      upload_ms,
    },
    size_bytes: data.length,
  };
}

function getDelayStatus(ms: number): { status: string; action: string } {
  if (ms < SYNC_TIME_OPTIMAL_MS) {
    return { status: 'optimal', action: 'none' };
  } else if (ms < SYNC_TIME_ACCEPTABLE_MS) {
    return { status: 'acceptable', action: 'none' };
  } else if (ms < SYNC_TIME_WARNING_MS) {
    return { status: 'warning', action: 'log_warning' };
  } else if (ms < SYNC_TIME_CRITICAL_MS) {
    return { status: 'slow', action: 'consider_archiving' };
  } else {
    return { status: 'critical', action: 'show_user_warning' };
  }
}

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);
    const userId = url.searchParams.get('user') || 'test';
    const dbKey = `${userId}/database.sqlite`;

    // Benchmark: Load database
    const loadResult = await loadDatabase(env.USER_DATA, dbKey);
    const db = loadResult.db;

    // Run a simple query
    const queryStart = performance.now();
    let tables: string[] = [];
    try {
      const result = db.exec("SELECT name FROM sqlite_master WHERE type='table'");
      if (result.length > 0) {
        tables = result[0].values.map(row => String(row[0]));
      }
    } catch (e) {
      // Empty database
    }
    const query_ms = performance.now() - queryStart;

    // Benchmark: Save database
    const saveResult = await saveDatabase(env.USER_DATA, dbKey, db);

    db.close();

    // Evaluate delay status (what the user experiences)
    const loadStatus = getDelayStatus(loadResult.timing.total_ms);
    const saveStatus = getDelayStatus(saveResult.timing.total_ms);

    return Response.json({
      success: true,

      // What matters: actual delays
      delay_analysis: {
        load: {
          ms: Math.round(loadResult.timing.total_ms),
          ...loadStatus,
        },
        save: {
          ms: Math.round(saveResult.timing.total_ms),
          ...saveStatus,
        },
        thresholds: {
          optimal_ms: SYNC_TIME_OPTIMAL_MS,
          acceptable_ms: SYNC_TIME_ACCEPTABLE_MS,
          warning_ms: SYNC_TIME_WARNING_MS,
          critical_ms: SYNC_TIME_CRITICAL_MS,
        },
      },

      // Detailed timing breakdown (for debugging)
      timing_breakdown: {
        load: {
          total_ms: Math.round(loadResult.timing.total_ms * 100) / 100,
          wasm_init_ms: Math.round(loadResult.timing.wasm_init_ms * 100) / 100,
          download_ms: Math.round(loadResult.timing.download_ms * 100) / 100,
          parse_ms: Math.round(loadResult.timing.parse_ms * 100) / 100,
        },
        query_ms: Math.round(query_ms * 100) / 100,
        save: {
          total_ms: Math.round(saveResult.timing.total_ms * 100) / 100,
          serialize_ms: Math.round(saveResult.timing.serialize_ms * 100) / 100,
          upload_ms: Math.round(saveResult.timing.upload_ms * 100) / 100,
        },
      },

      // Size info (for reference, not for thresholds)
      db_info: {
        size_kb: Math.round(saveResult.size_bytes / 1024 * 100) / 100,
        size_bytes: saveResult.size_bytes,
        tables,
      },
    });
  }
};
```

### 4. Test Locally

```bash
# Run local dev server
npx wrangler dev

# Test with existing user database
curl "http://localhost:8787?user=a"

# Expected output:
# {
#   "success": true,
#   "delay_analysis": {
#     "load": { "ms": 145, "status": "acceptable", "action": "none" },
#     "save": { "ms": 98, "status": "optimal", "action": "none" },
#     "thresholds": { "optimal_ms": 100, "acceptable_ms": 300, ... }
#   },
#   "timing_breakdown": {
#     "load": { "total_ms": 145.23, "download_ms": 87.5, "parse_ms": 52.1, ... },
#     "save": { "total_ms": 98.45, "serialize_ms": 12.3, "upload_ms": 85.2 }
#   },
#   "db_info": { "size_kb": 127.5, "tables": ["raw_clips", "projects", ...] }
# }
```

### 5. Test Under Different Conditions

The goal is to find when **delay** becomes a problem, not when size becomes a problem:

```bash
# Test your actual database
curl "http://localhost:8787?user=a"

# Run multiple times to see variance
for i in {1..10}; do curl -s "http://localhost:8787?user=a" | jq '.delay_analysis'; done

# Test from deployed Worker (real network latency)
npx wrangler deploy
curl "https://sqlite-poc.your-subdomain.workers.dev?user=a"
```

---

## Success Criteria

1. **sql.js loads successfully** in Cloudflare Workers environment
2. **Existing database** (user 'a') loads and queries work
3. **Delay thresholds validated** - do 100/300/500ms feel right?
4. **Timing breakdown** helps identify bottlenecks (network vs CPU)
5. **Edge cases identified** (empty DB, corrupted data, cold start)

---

## Deliverables

| Item | Description |
|------|-------------|
| Working POC | `workers-sqlite-poc/` directory with code |
| Delay measurements | Real timings from your database |
| Validated thresholds | Confirm 100/300/500/1000ms are appropriate |
| Bottleneck analysis | Is delay from network, parsing, or serialization? |

---

## After This Task

Once validated:
1. **Update Python backend** (`database.py`) to use delay-based thresholds instead of size-based
2. **Add sync timing** to Python backend - measure actual upload/download time
3. **Proceed with RunPod** (Task 05+) with confidence in data strategy
4. **Workers implementation** (Phase 3) has proven foundation

### Implementing Delay-Based Warnings in Python Backend

```python
# database.py additions

import time
from collections import deque

# Delay thresholds (validated by POC)
SYNC_TIME_WARNING_MS = 500
SYNC_TIME_CRITICAL_MS = 1000

# Track recent sync times
_recent_sync_times: deque = deque(maxlen=10)

def sync_db_to_cloud_with_timing():
    """Sync database and track timing for delay-based warnings."""
    if not R2_ENABLED:
        return

    start = time.perf_counter()

    # ... existing sync logic ...

    elapsed_ms = (time.perf_counter() - start) * 1000
    _recent_sync_times.append(elapsed_ms)

    if elapsed_ms > SYNC_TIME_CRITICAL_MS:
        logger.warning(
            f"DATABASE SYNC CRITICAL: Sync took {elapsed_ms:.0f}ms (>{SYNC_TIME_CRITICAL_MS}ms). "
            f"Consider archiving old data to Durable Objects."
        )
    elif elapsed_ms > SYNC_TIME_WARNING_MS:
        logger.info(
            f"Database sync slow: {elapsed_ms:.0f}ms - approaching critical threshold"
        )
```

---

## Notes

### sql.js in Workers Caveats

1. **WASM loading**: First request may be slower (WASM compilation)
2. **Memory limits**: Workers have 128MB default limit
3. **CPU limits**: Paid Workers have 30s CPU time (plenty for our use)
4. **No persistence**: Everything is in-memory, must save to R2

### Alternative: Cloudflare D1

D1 is Cloudflare's managed SQLite. We're not using it because:
- Less control over data location
- Can't easily migrate existing SQLite files
- R2 + sql.js gives us more flexibility

However, D1 could be reconsidered if sql.js has issues.
